{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('newdata.csv', index_col=0)\n",
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def rolling_averages(team, cols, new_cols, window=5):\n",
    "    team = team.sort_values(\"Date\")    # Getting team data organized chronologically\n",
    "    rolling = team[cols].rolling(window, closed='left').mean()   # closed=left to ignore current row in sliding window\n",
    "    team[new_cols] = rolling\n",
    "    team = team.dropna(subset=new_cols) # dropping first rows because not enough data\n",
    "    return team\n"
   ],
   "id": "75798beac165dea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_opp = data.drop(columns=['Opponent']).rename(columns={\n",
    "    'Team': 'Opponent',\n",
    "    'CF': 'Opponent_CF',\n",
    "    'CA': 'Opponent_CA',\n",
    "    'CF%': 'Opponent_CF%',\n",
    "    'FF': 'Opponent_FF',\n",
    "    'FA': 'Opponent_FA',\n",
    "    'FF%': 'Opponent_FF%',\n",
    "    'SF': 'Opponent_SF',\n",
    "    'SA': 'Opponent_SA',\n",
    "    'GF': 'Opponent_GF',\n",
    "    'GA': 'Opponent_GA',\n",
    "    'xGF': 'Opponent_xGF',\n",
    "    'xGA': 'Opponent_xGA',\n",
    "    'xGF%': 'Opponent_xGF%',\n",
    "    'HDCF' : 'Opponent_HDCF',\n",
    "    'HDCF%' : 'Opponent_HDCF%',\n",
    "    'SCF' : 'Opponent_SCF',\n",
    "    'PDO' : 'Opponent_PDO'\n",
    "})\n",
    "\n",
    "\n",
    "merged = data.merge(\n",
    "    data_opp,\n",
    "    left_on=['Date', 'Opponent'],\n",
    "    right_on=['Date', 'Opponent'],\n",
    "    how='inner',\n",
    "    suffixes=('', '_y')  # Avoids conflicts if any columns arenâ€™t renamed\n",
    ")"
   ],
   "id": "250915d678cac86c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged",
   "id": "8377c07a2f572be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged['CF_diff'] = merged['CF'] - merged['Opponent_CF']\n",
    "merged['CF%_diff'] = merged['CF%'] - merged['Opponent_CF%']\n",
    "merged['GF_diff'] = merged['GF'] - merged['Opponent_GF']\n",
    "merged['xGF_diff'] = merged['xGF'] - merged['Opponent_xGF']\n",
    "merged['HDCF_diff'] = merged['HDCF'] - merged['Opponent_HDCF']\n",
    "merged['HDCF%_diff'] = merged['HDCF%'] - merged['Opponent_HDCF%']\n",
    "merged['FF_diff'] = merged['FF'] - merged['Opponent_FF']\n",
    "merged['FF%_diff'] = merged['FF%'] - merged['Opponent_FF%']\n",
    "merged['SCF_diff'] = merged['SCF'] - merged['Opponent_SCF']\n",
    "merged['PDO_diff'] = merged['PDO'] - merged['Opponent_PDO']\n"
   ],
   "id": "a2bd8f1bf005785f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged",
   "id": "7655350b606f9410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged['GF%'] = pd.to_numeric(merged['GF%'], errors='coerce')\n",
    "merged['xGF%'] = pd.to_numeric(merged['xGF%'], errors='coerce')\n",
    "\n",
    "print(merged[['GF%', 'xGF%']].dtypes)"
   ],
   "id": "c79114c7751b8c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feature = [\n",
    "    'CF%', 'FF%', 'SF%', 'xGF%', 'SCF%', 'HDCF%', 'GF%', 'SH%', 'SV%', 'HDCA', 'xGA', 'PDO'\n",
    "]\n",
    "feature += ['CF', 'CA', 'FF', 'FA', 'SF', 'GA', 'GF', 'SCA', 'SCF', 'HDCF']\n",
    "feature_diff = ['CF_diff', 'CF%_diff', 'GF_diff', 'xGF_diff', 'HDCF_diff', 'HDCF%_diff', 'FF_diff', 'FF%_diff', 'SCF_diff', 'PDO_diff']\n",
    "\n",
    "features = feature + feature_diff\n",
    "\n",
    "features = ['SF%', 'PDO', 'PDO_diff', 'SV%', 'CF%', 'CF%_diff', 'FF%', 'FF%_diff', 'HDCF%', 'HDCF%_diff', 'SCF%', 'SCF_diff', 'GF%', 'xGF%']\n",
    "\n",
    "features = ['SCF%', 'xGF%', 'CF%_diff']\n",
    "\n",
    "#all_predictors = ['CF%', 'SF%', 'xGF%', 'SV%']\n",
    "\n",
    "new_cols = [f'{c}_rolling' for c in features]\n",
    "\n",
    "predictors = new_cols\n",
    "\n",
    "\n"
   ],
   "id": "f4d7865b73ffc061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(merged[features].dtypes)",
   "id": "cd6b1f15ad1502b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adjusting columns order for debugging / clarity\n",
    "columns = list(merged.columns)\n",
    "\n",
    "columns.remove('Date')\n",
    "columns.remove('Result')\n",
    "\n",
    "columns.insert(1, 'Date')\n",
    "columns.insert(2, 'Result')\n",
    "\n",
    "merged = merged[columns]\n",
    "merged"
   ],
   "id": "7b00289c9ca0beef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged = merged[merged['Team'] != 'Arizona Coyotes']\n",
    "merged = merged[merged['Opponent'] != 'Arizona Coyotes']\n",
    "merged"
   ],
   "id": "95e9f9fd804d7f14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged['Date'] = pd.to_datetime(merged['Date'])\n",
    "\n",
    "data = merged.groupby('Team').apply(lambda x: rolling_averages(x, features, new_cols, 3))\n",
    "data = data.droplevel('Team')\n",
    "data.index = range(data.shape[0])\n",
    "data"
   ],
   "id": "2a8bf03bafef34d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.drop(columns=['Result_y'], inplace=True)",
   "id": "77d527a2f1e38b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data",
   "id": "16e77be83af4c00e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#####\n",
    "# 1. Initialize Elo ratings\n",
    "initial_elo = 1500\n",
    "teams = data['Team'].unique()\n",
    "elo_ratings = {team: initial_elo for team in teams}\n",
    "\n",
    "elo_features = []\n",
    "\n",
    "# 2. Loop through each game and update ratings\n",
    "for idx, row in data.iterrows():\n",
    "    team = row['Team']\n",
    "    opponent = row['Opponent']\n",
    "    result = row['Result']  # 1 if win, 0 if loss\n",
    "\n",
    "    # Optional: home-ice advantage\n",
    "    team_elo = elo_ratings[team]\n",
    "    opponent_elo = elo_ratings[opponent]\n",
    "\n",
    "    # Store Elo features BEFORE the game\n",
    "    elo_features.append({\n",
    "        'team_elo': team_elo,\n",
    "        'opponent_elo': opponent_elo,\n",
    "        'elo_diff': team_elo - opponent_elo\n",
    "    })\n",
    "\n",
    "    # Calculate expected outcome\n",
    "    expected_win = 1 / (1 + 10 ** ((opponent_elo - team_elo) / 400))\n",
    "\n",
    "    # Elo update (K-factor can be tuned)\n",
    "    #k = 40\n",
    "\n",
    "    k = 30\n",
    "    change = k * (result - expected_win)\n",
    "    elo_ratings[team] += change\n",
    "    elo_ratings[opponent] -= change\n",
    "\n",
    "# Convert Elo features to DataFrame\n",
    "elo_df = pd.DataFrame(elo_features)\n",
    "\n",
    "# Merge with combined_team_view\n",
    "dataset = pd.concat([data.reset_index(drop=True), elo_df], axis=1)\n",
    "\n",
    "####\n",
    "new = ['team_elo', 'opponent_elo', 'elo_diff']\n",
    "predictors = predictors + new"
   ],
   "id": "3ca7ff69b1f87d25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Computation for scale_pos_weight\n",
    "class_counts = data['Result'].value_counts()\n",
    "\n",
    "count_class_0 = class_counts[0]\n",
    "count_class_1 = class_counts[1]\n",
    "\n",
    "print(f\"Losses (0): {count_class_0}\")\n",
    "print(f\"Wins   (1): {count_class_1}\")\n",
    "\n",
    "scale = count_class_0 / count_class_1"
   ],
   "id": "a571b4d7b3b2c9d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = XGBClassifier(scale_pos_weight = scale, random_state=10)\n",
    "\n",
    "# Defining Time Series Split\n",
    "TSS = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "test_model = RandomForestClassifier(random_state=10)\n",
    "\n",
    "lin = BaggingClassifier(LogisticRegression(random_state=10, solver='liblinear', penalty='l2', max_iter=1000))\n"
   ],
   "id": "4ab684c4853b82d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Function to make predictions given the data, input features and chosen model\n",
    "\n",
    "def make_predictions(data, predictors, model):\n",
    "    train = data[data['Date'] < '2024-04-19']\n",
    "    #train = train[train['Date'] > '2022-10-06']\n",
    "    test = data[data['Date'] > '2024-04-19']\n",
    "    model.fit(train[predictors], train['Result'])\n",
    "    preds = model.predict(test[predictors])\n",
    "    combined  = pd.DataFrame(dict(actual=test['Result'], prediction = preds), index=test.index)\n",
    "    precision = precision_score(test['Result'], preds)\n",
    "    return combined, precision"
   ],
   "id": "174d61473d2d4f06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Defining search space for GridSearchCV\n",
    "search_grid = {\n",
    "    'n_estimators': [50, 75, 100, 150],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [1, 5, 10]\n",
    "\n",
    "}\n",
    "\n",
    "alt_search_grid = {\n",
    "    'n_estimators' : [50, 100, 200, 500],\n",
    "    'max_depth' : [3, 6, 9],\n",
    "    'min_samples_split': [3, 5, 10]\n",
    "}\n",
    "\n",
    "lin_search_grid = {\n",
    "    # Logistic Regression hyperparameters (base_estimator__)\n",
    "    'estimator__C' : [0.5, 0.8, 1.0],\n",
    "    'n_estimators': [3, 5, 10, 50, 100],\n",
    "}\n",
    "\n",
    "GS = GridSearchCV(\n",
    "    estimator = lin,\n",
    "    param_grid = lin_search_grid,\n",
    "    scoring = 'neg_log_loss',\n",
    "    refit = True,\n",
    "    cv = TSS,\n",
    "    verbose= 4\n",
    ")\n",
    "\n",
    "training = dataset[dataset['Date'] < '2024-04-19']  # Training using 2021-2024 data\n",
    "#training = training[training['Date'] > '2022-10-06']\n",
    "testing = dataset[dataset['Date'] > '2024-04-19']   # Testing on most recent season (2024-2025)\n",
    "print(training.columns[training.columns.str.contains('Result')])"
   ],
   "id": "b808d390b9aaf50d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictors",
   "id": "1180db15e1320382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "5b9e1159ad2685cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(type(training['Result']))         # should be <class 'pandas.Series'>\n",
    "print(training['Result'].shape)        # should be (n_samples,)"
   ],
   "id": "db5b763d982fac4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GS.fit(training[predictors], training['Result'])     # Training",
   "id": "6e48637756a103db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GS.best_score_",
   "id": "b5789ab2ed51efff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_model = GS.best_estimator_\n",
    "new_model"
   ],
   "id": "90a2716f2fbdedd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "combined, precision = make_predictions(dataset, predictors, new_model)\n",
    "precision"
   ],
   "id": "9b35bfeea0977fe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "\n",
    "predictions = new_model.predict(testing[predictors])\n",
    "\n",
    "print(classification_report(testing['Result'], predictions))"
   ],
   "id": "f260ceb86803bcfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create DataFrame pairing features with their importances\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': predictors,\n",
    "    'Importance': new_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(importances.head(10))"
   ],
   "id": "13fa575dc0d72118",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(dataset['Result'].unique())\n",
    "print(training['Result'].value_counts())"
   ],
   "id": "9ac962499d0517a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(training[new_cols].info())\n",
    "print(training[new_cols].isna().sum())\n",
    "print(training[new_cols].describe())\n"
   ],
   "id": "df8367b348020219",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# Check class balance after preprocessing\n",
    "print(data['Result'].value_counts())\n",
    "\n",
    "# Check if model predicted any 1s\n",
    "print(np.unique(predictions, return_counts=True))\n",
    "\n",
    "# Look at rolling feature distribution\n",
    "print(training[new_cols].describe())"
   ],
   "id": "e8ff0c7197234fd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "combined = combined.merge(dataset[['Date', 'Team', 'Opponent', 'Result']], left_index=True, right_index=True)\n",
    "combined"
   ],
   "id": "dbbbda321fee2c19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final = combined.merge(combined, left_on=['Date', 'Team'], right_on=['Date', 'Opponent'])  # few games will drop due to rolling windows\n",
    "final"
   ],
   "id": "ab588f1229e93933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final[(final['prediction_x'] == 1) & (final['prediction_y'] == 0)]['actual_x'].value_counts()",
   "id": "3dde4e74c88f5b9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "533 / (533 + 382)",
   "id": "4e398bf5b3200d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(dataset['Result'].value_counts())",
   "id": "f22b38d05562d516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_pred(data, predictors, model):\n",
    "    train = data[data['Date'] < '2024-04-19']\n",
    "    test = data[data['Date'] > '2024-04-19']\n",
    "\n",
    "    model.fit(train[predictors], train['Result'])\n",
    "\n",
    "    preds = model.predict(test[predictors])\n",
    "    probs = model.predict_proba(test[predictors])[:, 1]  # Probability of class 1 (win)\n",
    "\n",
    "    combined = pd.DataFrame({\n",
    "        'actual': test['Result'],\n",
    "        'prediction': preds,\n",
    "        'win_probability': probs\n",
    "    }, index=test.index)\n",
    "\n",
    "    # Keep useful columns\n",
    "    combined = pd.concat([combined, test[['Team', 'Opponent', 'Date']]], axis=1)\n",
    "\n",
    "    precision = precision_score(test['Result'], preds)\n",
    "    return combined, precision"
   ],
   "id": "c5b5f520053d8e9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "combined, precision = make_pred(dataset, predictors, new_model)\n",
    "combined"
   ],
   "id": "35fe14da221a9f02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge team vs opponent predictions\n",
    "paired = combined.merge(\n",
    "    combined,\n",
    "    left_on=['Date', 'Team'],\n",
    "    right_on=['Date', 'Opponent'],\n",
    "    suffixes=('_team', '_opp')\n",
    ")\n",
    "\n",
    "# Filter out same-team merges (shouldn't happen if data is clean)\n",
    "paired = paired[paired['Team_team'] != paired['Team_opp']]\n",
    "paired"
   ],
   "id": "bdc64244484e0575",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Filter only valid pairings\n",
    "paired = paired[paired['Team_team'] != paired['Team_opp']]\n",
    "\n",
    "# Choose team with higher probability to win\n",
    "paired['predicted_winner'] = paired.apply(\n",
    "    lambda row: row['Team_team'] if row['win_probability_team'] > row['win_probability_opp'] else row['Team_opp'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Determine actual winner from true result\n",
    "paired['actual_winner'] = paired.apply(\n",
    "    lambda row: row['Team_team'] if row['actual_team'] == 1 else row['Team_opp'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "paired"
   ],
   "id": "233e0dea7c4e4a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Evaluate how accurate our prediction was\n",
    "paired['correct'] = paired['predicted_winner'] == paired['actual_winner']\n",
    "accuracy = paired['correct'].mean()\n",
    "print(f\"Match-level accuracy: {accuracy:.3f}\")"
   ],
   "id": "6f3e30d13b18802b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
